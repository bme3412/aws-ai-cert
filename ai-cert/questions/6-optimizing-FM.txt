1. Which AWS service can help store embeddings within vector databases? 

Amazon S3

Amazon DynamoDB

Amazon OpenSearch Service

Amazon Kinesis Data Streams

Correct
The correct answer is Amazon OpenSearch Service.

Amazon OpenSearch Service is a fully managed service that makes it convenient to deploy, operate, and scale OpenSearch clusters in the AWS Cloud. OpenSearch supports vector data types, which allows it to store and query embeddings efficiently. Other services mentioned, such as Amazon S3, DynamoDB, and Kinesis Data Streams, are not optimized for storing and querying vector data like embeddings.

--------

2. What is an example of a multi-step task that an agent can perform? 

Performing a basic mathematical calculation 

Converting a text file to PDF format

Doing a math calculation and converting the answer into a JSON format

Sending an email notification

Correct
The correct answer is Doing a math calculation and converting the answer into a JSON format.

Agents in the context of multi-step tasks are designed to handle complex, multi-step workflows. Doing a math calculation and converting the answer into a JSON format involves two distinct steps. It performs a mathematical calculation, then transforms the result into a specific data format (JSON). This showcases the agent's ability to handle multiple tasks in a sequential manner.

--------

3. You are part of a research team tasked with evaluating the performance of a new language model on various natural language processing tasks.
Which approach is most suitable for evaluating the performance of a foundation model in this situation? 

Human evaluation

Benchmark dataset

Both human evaluation and benchmark datasets

Retrieval-Augmented Generation (RAG)

Correct
The correct answer is both human evaluation and benchmark datasets.

Both human evaluation and benchmark datasets are valid approaches for evaluating the performance of a foundation model. Human evaluation involves subjective assessment by humans, while benchmark datasets provide objective, quantitative measures of performance.

------

4. Which methods can be used for fine-tuning a foundation model (FM)? (Choose THREE.) 

Instruction tuning

ROUGE

Transfer learning

Reinforcement learning from human feedback (RLHF)

Model pruning

BLEU

SUBMIT

Correct
That's correct! Instruction tuning, transfer learning, and RLHF are all methods for fine-tuning a model. Recall that ROUGE and BLEU are metrics used to evaluate the quality of a model's output and model pruning is used to reduce the size and computational complexity of machine learning models.

-------

5. You are part of a team working on fine-tuning a large language model for a specific domain. To ensure the model's performance is optimized for the target domain, you need to carefully prepare the dataset.
Which of the following steps is most critical in the fine-tuning data preparation process to ensure the model's specialization and accuracy in the target domain?

Labeling with accurate and relevant labels

Ensuring data governance and compliance with industry regulations

Checking for representativeness and addressing potential biases

Incorporating user or expert feedback 

Correct
That's correct! In the fine-tuning process, labeling the data with accurate and relevant labels is crucial for guiding the model's adjustments to specialize in the target domain.

------

6. You are evaluating the performance of a language generation model on various text generation tasks, such as machine translation, summarization, and open-ended text generation. To assess the quality of the generated text, you need to choose an appropriate evaluation metric that can capture the semantic similarity between the model's output and human-generated reference texts.
Based on the information provided, which evaluation metric would be the most suitable for this scenario?

ROUGE

BLEU

BERTScore

Perplexity

SUBMIT

Incorrect
That's incorrect. BERTScore is the most suitable metric for evaluating the semantic similarity between the language generation model's output and human-generated reference texts.

------
