Task Statement 3.1: Describe design considerations for applications that use
foundation models.
Objectives:
• Identify selection criteria to choose pre-trained models (for example, cost,
modality, latency, multi-lingual, model size, model complexity,
customization, input/output length).
• Understand the effect of inference parameters on model responses (for
example, temperature, input/output length).
• Define Retrieval Augmented Generation (RAG) and describe its business
applications (for example, Amazon Bedrock, knowledge base).
• Identify AWS services that help store embeddings within vector databases
(for example, Amazon OpenSearch Service, Amazon Aurora, Amazon
Neptune, Amazon DocumentDB [with MongoDB compatibility], Amazon
RDS for PostgreSQL). 
Explain the cost tradeoffs of various approaches to foundation model
customization (for example, pre-training, fine-tuning, in-context learning,
RAG).
• Understand the role of agents in multi-step tasks (for example, Agents for
Amazon Bedrock). 

Task Statement 3.2: Choose effective prompt engineering techniques.
Objectives:
• Describe the concepts and constructs of prompt engineering (for example,
context, instruction, negative prompts, model latent space).
• Understand techniques for prompt engineering (for example, chain-ofthought, zero-shot, single-shot, few-shot, prompt templates).
• Understand the benefits and best practices for prompt engineering (for
example, response quality improvement, experimentation, guardrails,
discovery, specificity and concision, using multiple comments).
• Define potential risks and limitations of prompt engineering (for example,
exposure, poisoning, hijacking, jailbreaking).

Task Statement 3.3: Describe the training and fine-tuning process for foundation
models.
Objectives:
• Describe the key elements of training a foundation model (for example,
pre-training, fine-tuning, continuous pre-training).
• Define methods for fine-tuning a foundation model (for example,
instruction tuning, adapting models for specific domains, transfer learning,
continuous pre-training).
• Describe how to prepare data to fine-tune a foundation model (for
example, data curation, governance, size, labeling, representativeness,
reinforcement learning from human feedback [RLHF]). 

Task Statement 3.4: Describe methods to evaluate foundation model performance.
Objectives:
• Understand approaches to evaluate foundation model performance (for
example, human evaluation, benchmark datasets).
• Identify relevant metrics to assess foundation model performance (for
example, Recall-Oriented Understudy for Gisting Evaluation [ROUGE],
Bilingual Evaluation Understudy [BLEU], BERTScore).
• Determine whether a foundation model effectively meets business
objectives (for example, productivity, user engagement, task engineering).