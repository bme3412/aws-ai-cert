Deep learning is a type of machine learning that uses algorithmic structures called neural networks. These are based upon the structure of the human brain. In our brains, brain cells called neurons form a complex network where they send electrical signals to each other to help us process information.

In deep learning models, we use software modules called nodes to simulate the behavior of neurons. Deep neural networks comprise layers of nodes, including an input layer, several hidden layers, and an output layer of nodes. Every node in the neural network autonomously assigns weights to each feature. Information flows through the network in a forward direction from input to output. During training, the difference between the predicted output and the actual output is then calculated. The weights of the neurons are repeatedly adjusted to minimize the error.

Deep learning can excel at tasks like image classification and natural language processing where there is a need to identify the complex relationship between data objects. In our fish example from earlier, we saw that we can train a machine model to recognize objects in an image, but training the model required a lot of human effort to label thousands of images. This is the way that image classification, object detection, and other forms of computer vision have traditionally operated in the past. The concept of deep learning with neural networks has existed for some time.

However, the required computing power wasn't visible for most businesses to obtain until the arrival of low-cost cloud computing. Because anyone can now readily use powerful computing resources in the cloud, neural networks have become the standard algorithmic approach to computer vision. A big advantage of deep learning models for computer vision is that they don't need the relevant features given to them. They can identify patterns in images and extract the important features on their own. However, we might need to give a deep learning model millions of pictures of fish before it can accurately detect and label a fish in an image. And the compute infrastructure to train a deep learning model repeatedly on such a large dataset is going to cost more than the traditional approach. The decision to use traditional machine learning or deep learning depends on the type of data you need to process.

Traditional machine learning algorithms will generally perform well and be efficient when it comes to identifying patterns from structured data and labeled data. Examples include classification and recommendation systems. For instance, a cell phone company can use ML to predict when a customer will change carriers based on previous customer churn data. On the other hand, deep learning solutions are more suitable for unstructured data like images, videos, and text.

Tasks for deep learning include image classification and natural language processing, where the is a need to identify the complex relationships between pixels and words. For example, a deep learning solution can analyze social media mentions or product feedback to determine user sentiment. Both types of machine learning use statistical algorithms, but only deep learning uses neural networks to simulate human intelligence. As we've seen, deep learning models self-learn patterns, so they don't require as much work on selecting and extracting features. However, their infrastructure costs are significantly higher.

Finally, let's finish this lesson by talking more about generative AI. Generative AI is accomplished by using deep learning models that are pre-trained on extremely large datasets containing strings of text or, in AI terms, sequences. They use transformer neural networks, which change an input sequence, in Gen AI known as prompt, into an output sequence, which is the response to your prompt. Neural networks process the elements of a sequence sequentially one word at a time. Transformers process the sequence in parallel, which speeds up the training and allows much bigger datasets to be used. Large language models contain many billions of features, which captures a wide range of human knowledge.

With all this training, large models are very flexible in the tasks they can perform. They outperform other ML approaches to natural language processing. They excel at understanding human language so they can read long articles and summarize them. They are also great at generating text that's similar to the way a human would. As a result, they are good at language translation and even writing original stories, letters, articles, and poetry. They even know computer programming languages and can write code for software developers. Here, I asked Amazon Bedrock to explain large language models. 