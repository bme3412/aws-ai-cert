As we've seen, the output of a machine learning model is an inference. In our fish example, we trained our model to recognize images of fish by training it with images of fish swimming in the water.

However, our deployed model might see images of fish that are a bit different, like a fish out of water, and not recognize them as fish. When a model performs better on training data than it does on new data, it is called overfitting, and it is said that the model does not recognize well. The model fits the training data too well, so when it sees something slightly different, it thinks the probability is low that it is a fish.

Usually, the best way to correct a model that is overfitting is to train it with data that is more diverse. Sometimes, if you train your model for too long, it will start to overemphasize unimportant features called noise, which is another way of overfitting.

Underfitting is a type of error that occurs when the model cannot determine a meaningful relationship between the input and output data. Underfit models give inaccurate results for both the training dataset and new data. This can happen if you haven't trained the model long enough or with a large enough dataset. Because training for too long can cause overfitting, data scientists try to find the sweet spot for training time where the model doesn't underfit or overfit.

Bias is when there are disparities in the performance of a model across different groups. The results are skewed in favor of or against an outcome for a particular class. As an example, consider a machine learning model for automatically improving loan applications. The training data, we have examples of loan applications that should be approved and those that should not. If the training data doesn't have enough applications from diverse population, the model could learn a pattern that is biased against a particular group.

For example, the loan application contains features like the customer's income, job history, age, gender, and location. Suppose that there aren't any approved applications from 25-year-old women living in Wisconsin in the training data. Then, the model could learn that those should not be approved, even though other features such as their income and job history would qualify them.

The quality of a model depends on the underlying data quality and quantity. Also, if a model is showing bias, the weight of features that are introducing noise can be directly adjusted by the data scientists. For example, it could completely remove gender consideration by the model. Fairness constraints, such as age and sex discrimination, should be identified at the beginning before creating a model. Training data should be inspected and evaluated for potential bias, and models need to be continually evaluated by checking their results for fairness. I'm going to pause this lesson here.