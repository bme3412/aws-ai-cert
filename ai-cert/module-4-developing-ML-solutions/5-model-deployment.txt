Model deployment is the integration of the model and its resources into a production environment so that it can be used to create predictions.

In a self-hosted API approach, you deploy and host your ML models on your own infrastructure, either on premises or in the cloud (using virtual machines or containers). This approach involves setting up and managing the necessary infrastructure, such as web servers, load balancers, and databases, to serve your ML models as APIs.
Managed API services are cloud-based services that provide a fully managed environment for deploying and hosting your ML models as APIs. SageMaker is an example. These services abstract away the underlying infrastructure management so you can focus on building and deploying your models.

Advantages of self-hosted APIs include greater control over the infrastructure, potential cost savings (depending on usage), and the ability to customize the deployment environment. However, this approach requires more operational overhead and responsibility for managing and maintaining the infrastructure.

SageMaker is a fully managed ML service. With SageMaker, data scientists and developers can quickly and confidently build, train, and deploy ML models into a production-ready, hosted environment. Within a few steps, you can deploy a model into a secure and scalable environment.

Deployment Options:
1. Real-time inference is ideal for inference workloads where you have real-time,   interactive, and low latency requirements.
2. Batch-Transform: Use batch transform when you need to get inferences from large datasets and don't need a persistent endpoint. You can also use it when you need to preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.
3. Asynchronous: SageMaker asynchronous inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements.
4. Serverless: On-demand serverless inference is ideal for workloads that have idle periods between traffic spurts and can tolerate cold starts. It is a purpose-built inference option that you can use to deploy and scale ML models without configuring or managing any of the underlying infrastructure.

MLOps:
MLOps refers to the practice of operationalizing and streamlining the end-to-end machine learning lifecycle from model development and deployment to monitoring and maintenance. It helps ensure that models are not just developed but also deployed, monitored, and retrained systematically and repeatedly.
Like DevOps, MLOps relies on a collaborative and streamlined approach to the machine learning development lifecycle. It is the intersection of people, process, and technology that optimizes the end-to-end activities required to develop, build, and operate machine learning workloads.

Applications that expose trained models might have different hosting requirements and strategies than standard applications. Trained models are sensitive to changes in data; therefore, a model-based application that works well when first implemented might not perform as well days, weeks, or months after being implemented. To account for these differences, you need different processes and procedures for applications that are based in managing ML.
MLOps accounts for the unique aspects of artificial intelligence and machine learning (AI/ML) projects in project management, continuous integration and delivery (CI/CD), and quality assurance. With it, you can improve delivery time, reduce defects, and make data science more productive.
A goal of MLOps is to get ML workloads into production and keep them operating. To meet this goal, MLOps adopts many DevOps principles and practices for the development, training, deployment, monitoring, and retraining of machine learning models. 

Key principles of MLOps
1. Version Control: For reproducibility, machine learning workflows must track changes to assets like data, code, and models. It can be rolled back to previous versions when needed.
Overall, version control and code review provide reproducible, trustworthy machine learning

2. Automation: For repeatability, consistency, and scalability, you can automate the various stages in the machine learning pipeline. This includes the data ingestion, pre-processing, model training, and validation and deployment stages.

3. CI/CD: Continuous integration extends the validation and testing of code to data and models in the pipeline.
Continuous delivery automatically deploys the newly trained model or model prediction service.

4. Model governance: Good governance of machine learning systems requires close collaboration between data scientists, engineers, and business stakeholders. Clear documentation, effective communication channels, and feedback mechanisms help align everyone and improve models over time. It is also crucial to protect sensitive data, secure access, and meet compliance rules. A structured process for reviewing, validating, and approving models before deployment checks for fairness, bias, and ethics. Governance manages all aspects of systems for efficiency.

AWS services for MLOps
1. SageMaker Data Wrangler is a LCNC tool that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data by using a web interface.
By using the SageMaker Processing API, data scientists can run scripts and notebooks to process, transform, and analyze datasets various ML frameworks such as scikit-learn, MXNet, or PyTorch while benefiting from fully managed machine learning environments.

2. SageMaker Feature Store helps data scientists, machine learning engineers, and general practitioners to create, share, and manage features for ML development.
3. Train: SageMaker provides a training job feature to train models using built-in algorithms or custom algorithms.
4. Experiments: Use SageMaker Experiments to experiment with multiple combinations of data, algorithms, and parameters, all while observing the impact of incremental changes on model accuracy. 
5. Processing job: SageMaker Processing refers to the capabilities to run data pre-processing and post-processing, feature engineering, and model evaluation tasks on the SageMaker fully managed infrastructure.
6. Registry: With SageMaker Model Registry you can catalog models, manage model versions, manage the approval status of a model, or deploy models to production.
7. Deployments:With SageMaker, you can deploy your ML models to make predictions, also known as inference. SageMaker provides a broad selection of ML infrastructure and model deployment options to help meet all your ML inference needs.
8. Monitor Model: With SageMaker Model Monitor, you can monitor the quality of SageMaker ML models in production.
9. Pipelines: You can use Amazon SageMaker Model Building Pipelines to create end-to-end workflows that manage and deploy SageMaker jobs
